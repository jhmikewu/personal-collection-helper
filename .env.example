# Emby Configuration
EMBY_URL=http://your-emby-server:8096
EMBY_API_KEY=your-emby-api-key

# Booklore Configuration
BOOKLORE_URL=http://your-booklore-server:6060
BOOKLORE_API_KEY=your-booklore-api-token

# Server Configuration
HOST=0.0.0.0
PORT=8090

# Logging
LOG_LEVEL=INFO

# LLM Configuration for Recommendations
# The provider field is just for documentation - any OpenAI-compatible API works
# Just set LLM_BASE_URL to your provider's endpoint

# OpenAI:
LLM_PROVIDER=openai
LLM_API_KEY=sk-your-openai-api-key
LLM_BASE_URL=
LLM_MODEL=gpt-4o-mini

# DeepSeek:
# LLM_PROVIDER=deepseek
# LLM_API_KEY=sk-your-deepseek-api-key
# LLM_BASE_URL=https://api.deepseek.com/v1
# LLM_MODEL=deepseek-chat

# Groq:
# LLM_PROVIDER=groq
# LLM_API_KEY=gsk-your-groq-api-key
# LLM_BASE_URL=https://api.groq.com/openai/v1
# LLM_MODEL=llama3.2-70b-vision-preview

# Anthropic Claude (different API format):
# LLM_PROVIDER=anthropic
# LLM_API_KEY=sk-ant-your-anthropic-api-key
# LLM_BASE_URL=
# LLM_MODEL=claude-3-5-haiku-20241022

# Ollama (local, different API format):
# LLM_PROVIDER=ollama
# LLM_API_KEY=
# LLM_BASE_URL=http://localhost:11434
# LLM_MODEL=llama3.2

# Any other OpenAI-compatible provider:
# Just set LLM_BASE_URL to the provider's endpoint
# LLM_PROVIDER=custom
# LLM_API_KEY=your-api-key
# LLM_BASE_URL=https://your-provider.com/v1
# LLM_MODEL=your-model-name

# Advanced LLM Settings (optional)
LLM_MAX_TOKENS=1000
LLM_TEMPERATURE=0.7
